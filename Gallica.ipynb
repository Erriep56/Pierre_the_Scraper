{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper plusieurs pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "choix = \"Dante\"\n",
    "\n",
    "identifiants = {\n",
    "    \"invalides\": \"btv1b84901123\",\n",
    "    \"mariette\": \"bpt6k15082862\",\n",
    "    \"titien_hiero\": \"btv1b53253107v\",\n",
    "    \"titien_AT_NT\":\"btv1b53249959m\",\n",
    "    \"titien_NT\":\"btv1b53253106d\",\n",
    "    \"theses_ita\":\"btv1b532635925\",\n",
    "    \"Bretagne\":\"btv1b10525728f\",\n",
    "    \"Dante\":\"bpt6k10448149\"\n",
    "}\n",
    "\n",
    "identifiant = identifiants.get(choix)\n",
    "\n",
    "page = 0\n",
    "\n",
    "# Répertoire de sauvegarde\n",
    "output_folder = f\"./images/{choix}/\"\n",
    "\n",
    "# Crée le répertoire si nécessaire\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Plage de pages\n",
    "start_page = 0\n",
    "end_page = 500\n",
    "\n",
    "# Délais et retries\n",
    "delay = 5\n",
    "modif_delay = 3\n",
    "max_retries = 3\n",
    "\n",
    "# Headers, ligne conseignée par chatGPT pour faire semblant qu'on est un navigateur\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Télécharger les images\n",
    "for page in range(start_page, end_page + 1):\n",
    "    url = f\"https://gallica.bnf.fr/iiif/ark:/12148/{identifiant}/f{page}/full/full/0/native.jpg\"\n",
    "    filename = f\"{output_folder}image_f{page}.jpg\"\n",
    "    attempts = 0\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"la page {page} existe déjà\")\n",
    "        continue\n",
    "\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            print(f\"Tentative de téléchargement de la page {page}\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                with open(filename, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(f\"Téléchargé : {filename}\")\n",
    "                break\n",
    "            elif response.status_code == 429:  # Trop de requêtes\n",
    "                attempts += 1\n",
    "                print(f\"Trop de requêtes, tentative {attempts} sur {max_retries}, réessaie dans {delay * modif_delay} secondes.\")\n",
    "                time.sleep(delay * modif_delay)\n",
    "            elif response.status_code == 404:\n",
    "                sys.exit(\"Erreur 404, arrêt du script.\")\n",
    "            else:\n",
    "                print(f\"Erreur {response.status_code} pour l'URL : {url}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur réseau : {e}\")\n",
    "            \n",
    "    time.sleep(delay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper 1 page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL de base\n",
    "base_url = \"https://gallica.bnf.fr/iiif/ark:/12148/bpt6k15082862/f{page}/full/full/0/native.jpg\"\n",
    "\n",
    "# Répertoire de sauvegarde (changez le chemin selon vos besoins)\n",
    "output_folder = \"./images/\"\n",
    "\n",
    "# Plage de pages\n",
    "lapage = 53\n",
    "delay = 15\n",
    "\n",
    "# Le header\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "# Télécharger les images\n",
    "\n",
    "def scraper(page):\n",
    "    url = base_url.format(page=page)\n",
    "    filename = f\"{output_folder}image_f{page}.jpg\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Téléchargé : {filename}\")\n",
    "        else:\n",
    "            print(f\"Erreur {response.status_code} pour l'URL : {url}\")\n",
    "            time.sleep(delay)\n",
    "            response = requests.get(url, headers=headers)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du téléchargement de {url} : {e}\")\n",
    "    \n",
    "    return\n",
    "\n",
    "scraper(lapage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vérification de la présence des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "page = 1\n",
    "page_max = 345\n",
    "\n",
    "output_folder = \"./images/\"\n",
    "\n",
    "page_perdue =[]\n",
    "\n",
    "for page in range(1, page_max + 1):\n",
    "    filename = f\"{output_folder}image_f{page}.jpg\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"L'image {page} n'existe pas.\")\n",
    "        page_perdue.append(f\"{page}\")\n",
    "\n",
    "print(f\"Les pages {page_perdue} n'ont pas été téléchargées.\")     \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
